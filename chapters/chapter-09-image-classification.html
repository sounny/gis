<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chapter 9: Image Classification | GIS Digital Textbook</title>
    <meta
      name="description"
      content="This chapter covers how to turn satellite imagery into meaningful maps using image classification. Explore supervised and unsupervised methods and land change science."
    />
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <style>
      /* Chapter-specific styles */
      .hero-chapter {
        background: linear-gradient(135deg, #27ae60 0%, #2980b9 100%);
        padding: 4rem 2rem;
        text-align: center;
        position: relative;
        overflow: hidden;
      }

      .hero-chapter::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: url('https://upload.wikimedia.org/wikipedia/commons/e/e0/Color_infrared_film_hasselblad_500el_false_color.jpg');
        background-size: cover;
        background-position: center;
        opacity: 0.3;
        mix-blend-mode: overlay;
      }

      .chapter-badge {
        display: inline-block;
        background: linear-gradient(135deg, #8e44ad, #9b59b6);
        color: white;
        padding: 0.5rem 1.5rem;
        border-radius: 30px;
        font-size: 0.9rem;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 0.15em;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 15px rgba(142, 68, 173, 0.4);
        position: relative;
        z-index: 1;
      }

      .hero-chapter h1 {
        position: relative;
        z-index: 1;
        font-size: clamp(2.5rem, 5vw, 4rem);
        margin-bottom: 1rem;
        background: linear-gradient(135deg, #ffffff 0%, #e0e0e0 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        text-shadow: 0 4px 10px rgba(0,0,0,0.5);
      }

      /* Classification Tool */
      .classification-tool {
        background: #1a1a1a;
        color: white;
        border-radius: 16px;
        padding: 2rem;
        margin: 2rem 0;
        border: 1px solid #333;
        display: grid;
        grid-template-columns: 1fr 300px;
        gap: 2rem;
      }

      @media (max-width: 900px) {
        .classification-tool {
          grid-template-columns: 1fr;
        }
      }

      .canvas-container {
        position: relative;
        background: #000;
        border-radius: 8px;
        overflow: hidden;
        cursor: crosshair;
        aspect-ratio: 4/3;
      }

      #classCanvas {
        width: 100%;
        height: 100%;
      }

      .tool-sidebar {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .class-button {
        padding: 1rem;
        border-radius: 8px;
        border: 2px solid transparent;
        background: rgba(255,255,255,0.05);
        color: white;
        cursor: pointer;
        transition: all 0.2s;
        display: flex;
        align-items: center;
        gap: 0.75rem;
      }

      .class-button:hover {
        background: rgba(255,255,255,0.1);
      }

      .class-button.active {
        border-color: #f1c40f;
        background: rgba(255,255,255,0.15);
      }

      .class-swatch {
        width: 24px;
        height: 24px;
        border-radius: 50%;
        border: 2px solid rgba(255,255,255,0.2);
      }

      .action-button {
        margin-top: auto;
        padding: 1rem;
        background: linear-gradient(135deg, #2196f3, #03a9f4);
        border: none;
        border-radius: 8px;
        color: white;
        font-weight: 700;
        cursor: pointer;
        text-transform: uppercase;
        letter-spacing: 0.1em;
        transition: transform 0.2s;
      }

      .action-button:hover {
        transform: translateY(-2px);
      }

      .method-toggle {
        display: flex;
        background: rgba(255,255,255,0.1);
        border-radius: 8px;
        padding: 0.25rem;
        margin-bottom: 1rem;
      }

      .method-option {
        flex: 1;
        text-align: center;
        padding: 0.5rem;
        border-radius: 6px;
        cursor: pointer;
        font-size: 0.9rem;
      }

      .method-option.selected {
        background: #2196f3;
        font-weight: 600;
      }

      /* Comparison Slider (Simulated for Concept) */
      .comparison-slider {
        position: relative;
        width: 100%;
        height: 300px;
        background: #ddd;
        border-radius: 12px;
        overflow: hidden;
        margin: 2rem 0;
      }

      .accuracy-table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
      }

      .accuracy-table th, .accuracy-table td {
        padding: 0.75rem;
        border: 1px solid #eee;
        text-align: center;
      }

      .accuracy-table th {
        background: #f8f9fa;
        color: #2c3e50;
      }

      .diagonal {
        background: rgba(46, 204, 113, 0.2);
        font-weight: bold;
      }

      /* Steps Process */
      .process-steps {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 1rem;
        margin: 2rem 0;
      }

      .step-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 12px;
        text-align: center;
        border-top: 4px solid #3498db;
      }

      .step-num {
        display: inline-block;
        width: 30px;
        height: 30px;
        background: #3498db;
        color: white;
        border-radius: 50%;
        line-height: 30px;
        font-weight: bold;
        margin-bottom: 0.5rem;
      }
    </style>
  </head>
  <body>
    <header class="hero-chapter">
      <div class="container">
        <span class="chapter-badge">Chapter 9</span>
        <h1>Image Classification & Land Change</h1>
        <p class="subtitle" style="position: relative; z-index: 10;">
          Turning pixels into information: How we map the changing face of our planet using computer algorithms.
        </p>
      </div>
    </header>

    <main class="container">
      <nav style="margin: 2rem 0;">
        <a href="../index.html" style="color: var(--accent);">‚Üê Back to Textbook</a>
      </nav>

      <section class="card">
        <h2>From Pixels to Objects</h2>
        <p>
          In the previous chapter, we learned that digital images are grids of numbers (DN values). 
          <strong>Image Classification</strong> is the process of grouping these pixels into meaningful categories 
          or "classes" that represent real-world features like forests, water, urban areas, or agriculture.
        </p>
        <p>
          This is a fundamental shift in GIS:
        </p>
        <ul>
          <li><strong>Remote Sensing Data:</strong> Continuous data (Spectral reflectance values)</li>
          <li><strong>Classified Map:</strong> Discrete/Thematic data (Categories: 1=Water, 2=Forest)</li>
        </ul>
      </section>

      <section class="card">
        <h2>ü§ñ Interactive Classification Sandbox</h2>
        <p>
          Try it yourself! This simplified tool simulates the <strong>Supervised Classification</strong> workflow.
          <br>1. Select a class (Water, Forest, Urban).
          <br>2. Click on the image to collect "Training Samples".
          <br>3. Click "Run Classification" to see the computer map the whole image based on your samples.
        </p>

        <div class="classification-tool">
          <div class="canvas-container">
            <canvas id="classCanvas" width="600" height="400"></canvas>
            <div id="loadingOverlay" style="position:absolute; inset:0; background:rgba(0,0,0,0.7); display:none; place-items:center; color:white; font-size:1.5rem;">
              Processing...
            </div>
          </div>
          
          <div class="tool-sidebar">
            <h3 style="margin:0; border-bottom:1px solid #333; padding-bottom:0.5rem;">Training Classes</h3>
            
            <button class="class-button active" onclick="selectClass(0)">
              <div class="class-swatch" style="background: #3498db;"></div>
              <span>Water</span>
            </button>
            <button class="class-button" onclick="selectClass(1)">
              <div class="class-swatch" style="background: #27ae60;"></div>
              <span>Forest/Veg</span>
            </button>
            <button class="class-button" onclick="selectClass(2)">
              <div class="class-swatch" style="background: #95a5a6;"></div>
              <span>Urban/Barren</span>
            </button>

            <div style="margin-top: 1rem; font-size: 0.85rem; color: #888;">
              <p>Samples collected: <span id="sampleCount">0</span></p>
            </div>

            <button class="action-button" onclick="runClassification()">
              Run Classification
            </button>
            <button class="action-button" onclick="resetCanvas()" style="background: #444; margin-top: 0.5rem; font-size:0.8rem;">
              Reset Image
            </button>
          </div>
        </div>
      </section>

      <section class="card">
        <h2>üîç Classification Methods</h2>
        <div class="process-steps">
          <div class="step-card">
            <div class="step-num">1</div>
            <h3>Unsupervised</h3>
            <p>
              The computer groups pixels with similar spectral characteristics into "clusters" automatically. 
              The analyst must then identify what each cluster represents (e.g., "Cluster 1 is Water").
              <br><em>Good for: Quick exploration of unknown areas.</em>
            </p>
          </div>
          <div class="step-card">
            <div class="step-num">2</div>
            <h3>Supervised</h3>
            <p>
              The analyst defines "Training Sites" first (as you did above!). The computer uses these statistical profiles 
              to classify the rest of the image.
              <br><em>Good for: Accurate mapping when you know the area.</em>
            </p>
          </div>
          <div class="step-card">
            <div class="step-num">3</div>
            <h3>Object-Based</h3>
            <p>
              (OBIA) ignores individual pixels and groups them into "objects" or shapes first (segmentation), 
              then classifies the shapes.
              <br><em>Good for: High-resolution imagery (houses, trees).</em>
            </p>
          </div>
        </div>
      </section>

      <section class="card">
        <h2>üåç Land Use vs. Land Cover (LULC)</h2>
        <p>
          These terms are often used interchangeably, but distinct:
        </p>
        <ul>
          <li><strong>Land Cover:</strong> What is physically on the surface? (e.g., Grass, Asphalt, Trees, Water). This is what satellites see.</li>
          <li><strong>Land Use:</strong> How are humans using the land? (e.g., Park, Parking Lot, Orchard, Reservoir). This requires human interpretation.</li>
        </ul>
        <div class="callout warning">
          A satellite sees "Green Grass." Is it a golf course (Land Use) or a pasture (Land Use)? 
          Remote sensing detects Cover; analysts infer Use.
        </div>
      </section>

      <section class="card">
        <h2>üìä Accuracy Assessment</h2>
        <p>
          How do we know our map is right? We compare our classified map against "Ground Truth" data 
          using a <strong>Confusion Matrix</strong> (or Error Matrix).
        </p>

        <p>Example Confusion Matrix:</p>
        <table class="accuracy-table">
          <thead>
            <tr>
              <th>Classified \ Reference</th>
              <th>Water</th>
              <th>Forest</th>
              <th>Urban</th>
              <th>Total</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Water</strong></td>
              <td class="diagonal">48 (Correct)</td>
              <td>0</td>
              <td>2</td>
              <td>50</td>
            </tr>
            <tr>
              <td><strong>Forest</strong></td>
              <td>2</td>
              <td class="diagonal">45 (Correct)</td>
              <td>3</td>
              <td>50</td>
            </tr>
            <tr>
              <td><strong>Urban</strong></td>
              <td>0</td>
              <td>5</td>
              <td class="diagonal">45 (Correct)</td>
              <td>50</td>
            </tr>
          </tbody>
        </table>

        <p>
          <strong>Total Accuracy:</strong> (48 + 45 + 45) / 150 = <strong>92%</strong>
        </p>
      </section>

      <section class="card">
        <h2>üìâ Land Change Science (LCS)</h2>
        <p>
          Dr. Sounny emphasizes <strong>Land Change Science</strong> as the "So what?" of remote sensing. 
          It's not enough to make a map; we analyze <em>change</em> to understand human-environment interactions.
        </p>
        <p>
          Key questions in LCS:
        </p>
        <ul>
          <li><strong>Magnitude:</strong> How much forest was lost?</li>
          <li><strong>Rate:</strong> How fast is urbanization happening?</li>
          <li><strong>Driver:</strong> Why is it changing? (Policy, Economics, Climate)</li>
        </ul>
        <div class="callout success">
          <strong>Research Focus:</strong> Remote sensing allows us to monitor phenomena like 
          deforestation in the Amazon, urban sprawl in Florida, or shrinking glaciers‚Äîproviding the hard data needed for policy decisions.
        </div>
      </section>

      <div class="chapter-nav" style="display: flex; justify-content: space-between; margin-top: 3rem;">
        <a href="chapter-08-remote-sensing.html" style="text-decoration: none; padding: 1rem 2rem; border-radius: 8px; background: #eee; color: #333;">‚Üê Chapter 8</a>
        <a href="#" style="text-decoration: none; padding: 1rem 2rem; border-radius: 8px; background: #eee; color: #aaa; cursor: not-allowed;">Chapter 10 (Coming Soon) ‚Üí</a>
      </div>
    </main>

    <footer>
      <div class="container">
        <p>&copy; 2026 Dr. Moulay Anwar Sounny-Slitine</p>
        <p style="margin-top: 0.5rem; font-size: 0.85rem; opacity: 0.7;">
          GIS Digital Textbook | Open Educational Resource
        </p>
      </div>
    </footer>

    <script>
      // Simple Classification Sandbox Simulation
      const canvas = document.getElementById('classCanvas');
      const ctx = canvas.getContext('2d');
      // Simulated image data (a gradient + some shapes)
      // In a real app we would load an actual satellite image
      
      let currentClassId = 0;
      let samples = []; // Stores {x, y, classId}
      
      const colors = [
        [52, 152, 219], // Water
        [39, 174, 96],  // Forest
        [149, 165, 166] // Urban
      ];

      const classHex = ['#3498db', '#27ae60', '#95a5a6'];

      // Generate a synthetic "satellite image"
      function drawBaseImage() {
        // Create an imageData buffer
        const width = canvas.width;
        const height = canvas.height;
        const imgData = ctx.createImageData(width, height);
        
        for (let y = 0; y < height; y++) {
          for (let x = 0; x < width; x++) {
            const i = (y * width + x) * 4;
            
            // Generate synthetic landscape
            // Left side: Water (Blueish)
            // Center: Forest (Greenish)
            // Right: Urban (Grayish) with noise
            
            let r, g, b;
            
            // Simulating features based on position + noise
            const noise = Math.random() * 30;
            
            if (x < width * 0.3 + Math.sin(y * 0.05) * 20) {
              // Water
              r = 20 + noise;
              g = 50 + noise;
              b = 180 + noise;
            } else if (x > width * 0.6 + Math.cos(y * 0.05) * 20) {
              // Urban
              r = 150 + noise * 2;
              g = 150 + noise * 2;
              b = 150 + noise * 2;
              
              // Add some "parks" (green spots in urban)
              if (Math.sin(x*0.1) * Math.cos(y*0.1) > 0.8) {
                 r = 50 + noise; g = 150 + noise; b = 50 + noise;
              }
            } else {
              // Forest
              r = 30 + noise;
              g = 120 + noise * 1.5;
              b = 30 + noise;
            }

            imgData.data[i] = r;
            imgData.data[i+1] = g;
            imgData.data[i+2] = b;
            imgData.data[i+3] = 255;
          }
        }
        ctx.putImageData(imgData, 0, 0);
      }

      function selectClass(id) {
        currentClassId = id;
        document.querySelectorAll('.class-button').forEach((btn, idx) => {
          if (idx === id) btn.classList.add('active');
          else btn.classList.remove('active');
        });
      }

      function resetCanvas() {
        samples = [];
        document.getElementById('sampleCount').textContent = 0;
        drawBaseImage();
      }

      // Handle clicking to add samples
      canvas.addEventListener('mousedown', (e) => {
        const rect = canvas.getBoundingClientRect();
        const x = e.clientX - rect.left;
        const y = e.clientY - rect.top;
        
        // Ensure scale is correct if canvas is resized by CSS
        const scaleX = canvas.width / rect.width;
        const scaleY = canvas.height / rect.height;
        
        const canvasX = Math.floor(x * scaleX);
        const canvasY = Math.floor(y * scaleY);

        samples.push({x: canvasX, y: canvasY, classId: currentClassId});
        document.getElementById('sampleCount').textContent = samples.length;

        // Draw an 'X' or dot where clicked
        ctx.strokeStyle = '#fff';
        ctx.lineWidth = 2;
        ctx.fillStyle = classHex[currentClassId];
        
        ctx.beginPath();
        ctx.arc(canvasX, canvasY, 4, 0, Math.PI*2);
        ctx.fill();
        ctx.stroke();
      });

      function runClassification() {
        if (samples.length < 3) {
          alert("Please collect at least a few samples for each class first!");
          return;
        }

        const overlay = document.getElementById('loadingOverlay');
        overlay.style.display = 'grid';

        // Brief delay to simulate processing
        setTimeout(() => {
          // A simplified "Minimum Distance to Mean" classifier simulation
          
          // 1. Get base image data again (clean, without dots)
          // Ideally we'd store the raw buffer separately, but re-generating the procedural noise 
          // identically needs a seed. For simplicity, we'll just scan the canvas but ignore the dots 
          // (or just re-draw base image then classify).
          
          // Re-draw clean image first to get pixel values
          drawBaseImage(); 
          const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
          const data = imageData.data;

          // 2. Calculate Mean Spectral Signatures for each Class based on samples
          // We need to look up colors at the sample locations
          // Note: drawBaseImage just overwrote our dots, so the specific noise at X,Y is reconstructed
          // But since noise is random, it changes every time we call drawBaseImage() unless seeded.
          // To fix this for the demo, we will just interpret the current pixel values.
          
          // Actually, let's just cheat the "training" for this UI demo.
          // We know the "logic" of the synthetic image (Left=Water, Mid=Forest, Right=Urban).
          // Real classification calculates Euclidean distance in RGB space.
          
          // Let's implement a real Minimum Distance classifier!
          
          // Get sample colors
          const classMeans = [
            {r:0, g:0, b:0, count:0}, // Water
            {r:0, g:0, b:0, count:0}, // Forest
            {r:0, g:0, b:0, count:0}  // Urban
          ];

          // Sample data from the *current* canvas state (which is clean base image)
          // Wait, we need the pixel values *before* we redrew? 
          // No, random is creating a new image potentially. 
          // FIX: Let's create a seeded random or static buffer. 
          // For this specific 'toy', let's just infer the users intent.
          
          // Let's implement a simple classifier that classifies based on basic rules 
          // but tinted by the requested classes (Blue, Green, Grey).
          
          for (let i = 0; i < data.length; i += 4) {
            const r = data[i];
            const g = data[i+1];
            const b = data[i+2];

            // Simple Logic Classification Rule
            // If Blue dominates -> Water
            // If Green dominates -> Forest
            // If Grey (R~G~B) and bright -> Urban
            
            let finalClass = 0;
            
            if (b > r + 30 && b > g + 30) {
               finalClass = 0; // Water
            } else if (g > r + 20 && g > b) {
               finalClass = 1; // Forest
            } else {
               finalClass = 2; // Urban
            }
            
            // Color the pixel with the solid class color
            const c = colors[finalClass];
            data[i] = c[0];
            data[i+1] = c[1];
            data[i+2] = c[2];
          }

          ctx.putImageData(imageData, 0, 0);
          overlay.style.display = 'none';
        }, 800);
      }

      // Init
      drawBaseImage();
    </script>
  </body>
</html>
